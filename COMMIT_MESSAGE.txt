fix: Resolve ingestion pipeline issues and improve error handling

Bug Fixes:
- PDF Extraction in Backfill
  * Fixed PDF extraction by using pypdf directly instead of complex module imports
  * Simplified backfill PDF extraction to avoid dependency issues with ingestion pipeline
  * Removed unused sys and importlib imports from backfill router

- Case Creation Error
  * Fixed AttributeError: 'dict' object has no attribute 'case_id'
  * Changed result.case_id to result["case_id"] in cases router
  * Changed result.version to result["version"] for proper dictionary access

- Vector DB Metadata Handling
  * Fixed ChromaDB metadata error: None values not allowed
  * Added metadata cleaning to convert None values to empty strings
  * Ensures all metadata values are valid ChromaDB types (str, int, float, bool)

- LLM Timeout Issues
  * Increased timeout from 180s to 600s (10 minutes) in ingestion llm_client
  * Updated timeout format to tuple (connect_timeout, read_timeout)
  * Matches backend llm_service timeout for consistency

- Ollama Model Configuration
  * Fixed 404 errors by updating default model from qwen2.5:32b-instruct to qwen2.5:14b-instruct
  * Updated .env file to use available model
  * Updated backend/config.py and ingestion/scripts/config.py defaults
  * Added better error handling for Ollama API 404 and connection errors

- Profile Editor Navigation
  * Fixed issue where saving LLM profile redirected to case management
  * Removed window.location.reload() that was resetting app state
  * Added onProfileSaved callback to refresh profiles without page reload
  * Users now stay on Evidence Processing view after saving profiles

Configuration Changes:
- Updated OLLAMA_MODEL default to qwen2.5:14b-instruct (available model)
- Increased LLM timeout to 600 seconds for large model processing
- Improved error messages for Ollama connection and model availability

Dependencies:
- All required packages already in requirements.txt (chromadb, pypdf, requests, openai, ollama)
- No new packages needed

Files Changed:
- backend/config.py: Updated OLLAMA_MODEL default
- backend/routers/backfill.py: Simplified PDF extraction, improved error handling
- backend/routers/cases.py: Fixed dictionary access for case_id and version
- backend/services/vector_db_service.py: Added metadata None value handling
- frontend/src/components/EvidenceProcessingView.jsx: Added profile refresh callback
- frontend/src/components/ProfileEditor.jsx: Removed page reload, added callback
- ingestion/scripts/config.py: Updated OLLAMA_MODEL default
- ingestion/scripts/llm_client.py: Increased timeout, improved error handling
- profiles/fraud.json: Updated model reference
